{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmnKBNn2wXWN",
        "outputId": "8e9e8d04-47ac-46b6-ef95-ec2e844c1a86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gsx_2JCwNXO",
        "outputId": "21c38dcd-6373-4c62-8be7-22037d86ffea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-172' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2987199528.py\", line 336, in start_server\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🚀 SERVIDOR LISTO\n",
            "============================================================\n",
            "📡 URL Local: http://localhost:8000\n",
            "🌐 URL Pública: NgrokTunnel: \"https://36760803ed93.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "📋 Documentación: NgrokTunnel: \"https://36760803ed93.ngrok-free.app\" -> \"http://localhost:8000\"/docs\n",
            "❤️ Health Check: NgrokTunnel: \"https://36760803ed93.ngrok-free.app\" -> \"http://localhost:8000\"/health\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [722]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2800:a4:1609:ec00:59a7:341c:eece:37e2:0 - \"POST /resumir HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [722]\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import nest_asyncio\n",
        "import time\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "from typing import List, Optional\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gc\n",
        "\n",
        "# Configuraciones\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Evitar warnings\n",
        "\n",
        "# Logging mejorado\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('/content/resumen_server.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Modelos Pydantic\n",
        "class Texto(BaseModel):\n",
        "    contenido: str = Field(..., min_length=10, max_length=10000)\n",
        "\n",
        "class TextosBatch(BaseModel):\n",
        "    contenidos: List[str] = Field(..., min_items=1, max_items=20)\n",
        "\n",
        "class ResumenResponse(BaseModel):\n",
        "    resumen: str\n",
        "    tiempo_procesamiento: float\n",
        "    tokens_procesados: int\n",
        "\n",
        "class ResumenesBatchResponse(BaseModel):\n",
        "    resumenes: List[str]\n",
        "    tiempo_total: float\n",
        "    articulos_procesados: int\n",
        "\n",
        "# Configuración de la aplicación\n",
        "app = FastAPI(\n",
        "    title=\"Servicio de Resumen Optimizado\",\n",
        "    description=\"API optimizada para resúmenes de noticias sobre cambio climático\",\n",
        "    version=\"2.0.0\"\n",
        ")\n",
        "\n",
        "# CORS mejorado\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Pool de threads para procesamiento paralelo\n",
        "executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "class OptimizedSummarizer:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/bart-large-cnn\"\n",
        "        self.summarizer = None\n",
        "        self.tokenizer = None\n",
        "        self.device = 0 if torch.cuda.is_available() else -1  # GPU si disponible\n",
        "        self.max_input_length = 1024\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            logger.info(\"🔄 Cargando modelo de resumen...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "            self.summarizer = pipeline(\n",
        "                \"summarization\",\n",
        "                model=self.model_name,\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=self.device,\n",
        "                framework=\"pt\",\n",
        "                clean_up_tokenization_spaces=True\n",
        "            )\n",
        "\n",
        "            load_time = time.time() - start_time\n",
        "            logger.info(f\"✅ Modelo cargado en {load_time:.2f} segundos usando {'GPU' if self.device != -1 else 'CPU'}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Error cargando modelo: {e}\")\n",
        "            raise\n",
        "\n",
        "    def preprocess_text(self, texto: str) -> str:\n",
        "        texto = texto.strip()\n",
        "        lineas = [linea.strip() for linea in texto.split('\\n') if len(linea.strip()) > 20]\n",
        "        texto_limpio = ' '.join(lineas)\n",
        "        tokens = self.tokenizer.encode(texto_limpio, truncation=True, max_length=self.max_input_length)\n",
        "        return self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "\n",
        "    def generate_summary(self, texto: str):\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            texto = self.preprocess_text(texto)\n",
        "            tokens = self.tokenizer.encode(texto)\n",
        "            resumen = self.summarizer(\n",
        "                texto,\n",
        "                max_length=130,\n",
        "                min_length=30,\n",
        "                do_sample=False\n",
        "            )[0]['summary_text']\n",
        "            resumen_final = self.postprocess_summary(resumen)\n",
        "            return {\n",
        "                \"resumen\": resumen_final,\n",
        "                \"tiempo_procesamiento\": time.time() - start_time,  # Fixed key name\n",
        "                \"tokens_procesados\": len(tokens)  # Fixed key name\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generando resumen: {e}\")\n",
        "            return {\n",
        "                \"resumen\": f\"Error al generar resumen: {str(e)}\",\n",
        "                \"tiempo_procesamiento\": 0,\n",
        "                \"tokens_procesados\": 0\n",
        "            }\n",
        "\n",
        "    def postprocess_summary(self, resumen: str) -> str:\n",
        "        if not resumen.endswith('.'):\n",
        "            resumen += '.'\n",
        "        if resumen:\n",
        "            resumen = resumen[0].upper() + resumen[1:]\n",
        "        return resumen\n",
        "\n",
        "    def generate_batch_summaries(self, textos: List[str]) -> List[dict]:\n",
        "        futures = [executor.submit(self.generate_summary, texto) for texto in textos]\n",
        "        resultados = []\n",
        "        for future in futures:\n",
        "            try:\n",
        "                resultados.append(future.result(timeout=30))\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error en batch processing: {e}\")\n",
        "                resultados.append({\n",
        "                    \"resumen\": f\"Error: {str(e)}\",\n",
        "                    \"tiempo_procesamiento\": 0,\n",
        "                    \"tokens_procesados\": 0\n",
        "                })\n",
        "        return resultados\n",
        "\n",
        "\n",
        "# Instancia global del resumidor\n",
        "logger.info(\"🚀 Inicializando servicio de resumen...\")\n",
        "summarizer_service = OptimizedSummarizer()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Endpoint de salud\"\"\"\n",
        "    return {\n",
        "        \"status\": \"active\",\n",
        "        \"service\": \"Servicio de Resumen Optimizado\",\n",
        "        \"version\": \"2.0.0\",\n",
        "        \"model\": \"facebook/bart-large-cnn\",\n",
        "        \"timestamp\": time.time()\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Check de salud detallado\"\"\"\n",
        "    try:\n",
        "        # Test rápido del modelo\n",
        "        test_result = summarizer_service.generate_summary(\n",
        "            \"Esta es una prueba del sistema de resumen automático para verificar que funciona correctamente.\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"status\": \"healthy\",\n",
        "            \"model_loaded\": summarizer_service.summarizer is not None,\n",
        "            \"test_time\": test_result[\"tiempo_procesamiento\"],\n",
        "            \"memory_usage\": f\"{torch.cuda.memory_allocated() / 1024**2:.1f}MB\" if torch.cuda.is_available() else \"CPU mode\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"unhealthy\", \"error\": str(e)}\n",
        "\n",
        "@app.post(\"/resumir\", response_model=ResumenResponse)\n",
        "async def resumir(texto: Texto):\n",
        "    \"\"\"Endpoint principal para resumir un texto individual\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"📝 Procesando texto de {len(texto.contenido)} caracteres\")\n",
        "\n",
        "        # Generar resumen\n",
        "        resultado = summarizer_service.generate_summary(texto.contenido)\n",
        "        return ResumenResponse(**resultado)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Error en /resumir: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error procesando resumen: {str(e)}\")\n",
        "\n",
        "@app.post(\"/resumir_batch\", response_model=ResumenesBatchResponse)\n",
        "async def resumir_batch(req: TextosBatch):\n",
        "    start_time = time.time()\n",
        "    resultados = summarizer_service.generate_batch_summaries(req.contenidos)\n",
        "    return ResumenesBatchResponse(\n",
        "        resumenes=[r[\"resumen\"] for r in resultados],\n",
        "        tiempo_total=time.time() - start_time,\n",
        "        articulos_procesados=len(resultados)\n",
        "    )\n",
        "\n",
        "@app.post(\"/resumir_parrafos\")\n",
        "async def resumir_parrafos(texto: Texto):\n",
        "    \"\"\"Endpoint para resumir párrafo por párrafo (método original mejorado)\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Dividir en párrafos y filtrar\n",
        "        parrafos = [p.strip() for p in texto.contenido.split('\\n') if len(p.strip()) > 20]\n",
        "\n",
        "        if not parrafos:\n",
        "            return {\"resumen\": \"No se encontraron párrafos válidos para resumir.\"}\n",
        "\n",
        "        logger.info(f\"📄 Procesando {len(parrafos)} párrafos\")\n",
        "\n",
        "        # Procesar párrafos en paralelo\n",
        "        resumenes_parrafos = summarizer_service.generate_batch_summaries(parrafos)\n",
        "\n",
        "        # Unir resúmenes\n",
        "        resumen_final = \"\\n\\n\".join([r[\"resumen\"] for r in resumenes_parrafos])\n",
        "\n",
        "        tiempo_total = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"resumen\": resumen_final,\n",
        "            \"parrafos_procesados\": len(parrafos),\n",
        "            \"tiempo_total\": tiempo_total\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Error en /resumir_parrafos: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error procesando párrafos: {str(e)}\")\n",
        "\n",
        "@app.get(\"/stats\")\n",
        "async def get_stats():\n",
        "    \"\"\"Estadísticas del servicio\"\"\"\n",
        "    return {\n",
        "        \"model_name\": summarizer_service.model_name,\n",
        "        \"device\": \"CPU\" if summarizer_service.device == -1 else f\"GPU:{summarizer_service.device}\",\n",
        "        \"max_input_length\": summarizer_service.max_input_length,\n",
        "        \"executor_threads\": executor._max_workers,\n",
        "        \"memory_info\": {\n",
        "            \"cuda_available\": torch.cuda.is_available(),\n",
        "            \"cuda_memory\": f\"{torch.cuda.memory_allocated() / 1024**2:.1f}MB\" if torch.cuda.is_available() else \"N/A\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Manejo de memoria\n",
        "@app.post(\"/clear_cache\")\n",
        "async def clear_cache():\n",
        "    \"\"\"Limpia la caché para liberar memoria\"\"\"\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return {\"status\": \"Cache cleared successfully\"}\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "# Funciones de utilidad para servidor\n",
        "def find_free_port(start_port: int = 8000, max_attempts: int = 10) -> int:\n",
        "    \"\"\"Encuentra un puerto libre\"\"\"\n",
        "    import socket\n",
        "    for port in range(start_port, start_port + max_attempts):\n",
        "        try:\n",
        "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "                s.bind(('0.0.0.0', port))\n",
        "                logger.info(f\"✅ Puerto {port} disponible\")\n",
        "                return port\n",
        "        except OSError:\n",
        "            logger.warning(f\"⚠️ Puerto {port} ocupado\")\n",
        "    raise RuntimeError(\"No se encontró puerto libre\")\n",
        "\n",
        "def setup_ngrok_with_port(port: int):\n",
        "    \"\"\"Configura ngrok con manejo de errores mejorado\"\"\"\n",
        "    try:\n",
        "        # Tu token de ngrok\n",
        "        ngrok.set_auth_token(\"2xEermx5e5Clyf3vVih326xdKKb_7oaPNcL5tDaT2TLy9vBVF\")\n",
        "\n",
        "        # Configurar túnel\n",
        "        public_url = ngrok.connect(port)\n",
        "        logger.info(f\"🌐 URL pública de ngrok: {public_url}\")\n",
        "\n",
        "        # Guardar URL para referencia\n",
        "        with open('/content/ngrok_url.txt', 'w') as f:\n",
        "            f.write(str(public_url))\n",
        "\n",
        "        # Mostrar información útil\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"🚀 SERVIDOR LISTO\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"📡 URL Local: http://localhost:{port}\")\n",
        "        print(f\"🌐 URL Pública: {public_url}\")\n",
        "        print(f\"📋 Documentación: {public_url}/docs\")\n",
        "        print(f\"❤️ Health Check: {public_url}/health\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        return public_url\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Error configurando ngrok: {e}\")\n",
        "        return None\n",
        "\n",
        "def start_server():\n",
        "    \"\"\"Inicia el servidor con configuración optimizada\"\"\"\n",
        "    try:\n",
        "        logger.info(\"🔧 Configurando servidor...\")\n",
        "\n",
        "        # Encontrar puerto libre\n",
        "        port = find_free_port(8000, 10)\n",
        "\n",
        "        # Configurar ngrok\n",
        "        logger.info(f\"🌐 Configurando ngrok en puerto {port}...\")\n",
        "        ngrok_url = setup_ngrok_with_port(port)\n",
        "\n",
        "        if not ngrok_url:\n",
        "            logger.warning(\"⚠️ No se pudo configurar ngrok, solo acceso local disponible\")\n",
        "\n",
        "        # Configurar uvicorn\n",
        "        config = uvicorn.Config(\n",
        "            app,\n",
        "            host=\"0.0.0.0\",\n",
        "            port=port,\n",
        "            log_level=\"info\",\n",
        "            access_log=True,\n",
        "            reload=False,\n",
        "            workers=1  # Un solo worker para evitar problemas con el modelo\n",
        "        )\n",
        "\n",
        "        server = uvicorn.Server(config)\n",
        "\n",
        "        logger.info(f\"🚀 Iniciando servidor en puerto {port}...\")\n",
        "        server.run()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.info(\"🛑 Servidor detenido por el usuario\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Error fatal del servidor: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        # Limpiar recursos\n",
        "        try:\n",
        "            ngrok.disconnect(ngrok.get_tunnels()[0].public_url)\n",
        "            ngrok.kill()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Middleware para logging de requests\n",
        "@app.middleware(\"http\")\n",
        "async def log_requests(request, call_next):\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = await call_next(request)\n",
        "\n",
        "    process_time = time.time() - start_time\n",
        "    logger.info(f\"📊 {request.method} {request.url.path} - {response.status_code} - {process_time:.3f}s\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Punto de entrada\n",
        "if __name__ == \"__main__\":\n",
        "    # Configurar asyncio para Colab\n",
        "    try:\n",
        "        nest_asyncio.apply()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    start_server()"
      ]
    }
  ]
}