{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmnKBNn2wXWN",
        "outputId": "4372300b-1b8c-4c98-8635-0499babaaa3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gsx_2JCwNXO",
        "outputId": "97ccfd46-dd48-4860-8867-83f40eff67bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-238' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-848922167.py\", line 411, in start_server\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "🚀 SERVIDOR LISTO\n",
            "============================================================\n",
            "📡 URL Local: http://localhost:8000\n",
            "🌐 URL Pública: NgrokTunnel: \"https://8efcd9ae6d70.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "📋 Documentación: NgrokTunnel: \"https://8efcd9ae6d70.ngrok-free.app\" -> \"http://localhost:8000\"/docs\n",
            "❤️ Health Check: NgrokTunnel: \"https://8efcd9ae6d70.ngrok-free.app\" -> \"http://localhost:8000\"/health\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [476]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2800:a4:167d:1100:e1aa:1ccc:ba1b:f7f6:0 - \"POST /resumir HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Error generando resumen: Already borrowed\n",
            "ERROR:__main__:Error generando resumen: Already borrowed\n",
            "ERROR:__main__:Error generando resumen: Already borrowed\n",
            "ERROR:__main__:Error generando resumen: Already borrowed\n",
            "ERROR:__main__:Error en batch processing: \n",
            "ERROR:__main__:Error en batch processing: \n",
            "ERROR:__main__:Error en batch processing: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2800:a4:167d:1100:e1aa:1ccc:ba1b:f7f6:0 - \"POST /resumir_batch HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import nest_asyncio\n",
        "import time\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "from typing import List, Optional\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gc\n",
        "\n",
        "# Configuraciones\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Evitar warnings\n",
        "\n",
        "# Logging mejorado\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('/content/resumen_server.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Modelos Pydantic\n",
        "class Texto(BaseModel):\n",
        "    contenido: str = Field(..., min_length=10, max_length=10000)\n",
        "\n",
        "class TextosBatch(BaseModel):\n",
        "    contenidos: List[str] = Field(..., min_items=1, max_items=20)\n",
        "\n",
        "class ResumenResponse(BaseModel):\n",
        "    resumen: str\n",
        "    tiempo_procesamiento: float\n",
        "    tokens_procesados: int\n",
        "\n",
        "class ResumenesBatchResponse(BaseModel):\n",
        "    resumenes: List[str]\n",
        "    tiempo_total: float\n",
        "    articulos_procesados: int\n",
        "\n",
        "# Configuración de la aplicación\n",
        "app = FastAPI(\n",
        "    title=\"Servicio de Resumen Optimizado\",\n",
        "    description=\"API optimizada para resúmenes de noticias sobre cambio climático\",\n",
        "    version=\"2.0.0\"\n",
        ")\n",
        "\n",
        "# CORS mejorado\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Pool de threads para procesamiento paralelo\n",
        "executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "class OptimizedSummarizer:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/bart-large-cnn\"\n",
        "        self.summarizer = None\n",
        "        self.tokenizer = None\n",
        "        self.device = -1  # Usar CPU por defecto en Colab\n",
        "        self.max_input_length = 1024\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Carga el modelo de manera optimizada\"\"\"\n",
        "        try:\n",
        "            logger.info(\"🔄 Cargando modelo de resumen...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Cargar tokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "            # Cargar modelo con configuraciones optimizadas\n",
        "            self.summarizer = pipeline(\n",
        "                \"summarization\",\n",
        "                model=self.model_name,\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=self.device,\n",
        "                framework=\"pt\",\n",
        "                clean_up_tokenization_spaces=True\n",
        "            )\n",
        "\n",
        "            load_time = time.time() - start_time\n",
        "            logger.info(f\"✅ Modelo cargado en {load_time:.2f} segundos\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Error cargando modelo: {e}\")\n",
        "            raise\n",
        "\n",
        "    def preprocess_text(self, texto: str) -> str:\n",
        "        \"\"\"Preprocesa el texto para mejor calidad de resumen\"\"\"\n",
        "        # Limpiar texto\n",
        "        texto = texto.strip()\n",
        "\n",
        "        # Remover líneas muy cortas o repetitivas\n",
        "        lineas = [linea.strip() for linea in texto.split('\\n') if len(linea.strip()) > 20]\n",
        "        texto_limpio = ' '.join(lineas)\n",
        "\n",
        "        # Truncar si es necesario\n",
        "        tokens = self.tokenizer.encode(texto_limpio, truncation=True, max_length=self.max_input_length)\n",
        "        texto_final = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "\n",
        "        return texto_final\n",
        "\n",
        "    def generate_summary(self, texto: str) -> dict:\n",
        "        \"\"\"Genera un resumen optimizado\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Preprocesar texto\n",
        "            texto_procesado = self.preprocess_text(texto)\n",
        "\n",
        "            if len(texto_procesado.strip()) < 50:\n",
        "                return {\n",
        "                    \"resumen\": \"Texto demasiado corto para resumir.\",\n",
        "                    \"tiempo\": time.time() - start_time,\n",
        "                    \"tokens\": 0\n",
        "                }\n",
        "\n",
        "            # Calcular longitudes dinámicamente\n",
        "            texto_length = len(texto_procesado.split())\n",
        "            max_length = min(180, max(60, texto_length // 3))\n",
        "            min_length = min(60, max(30, texto_length // 6))\n",
        "\n",
        "            # Generar resumen con parámetros optimizados\n",
        "            resumen_result = self.summarizer(\n",
        "                texto_procesado,\n",
        "                max_length=max_length,\n",
        "                min_length=min_length,\n",
        "                do_sample=False,\n",
        "                length_penalty=1.5,\n",
        "                no_repeat_ngram_size=3,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            resumen = resumen_result[0]['summary_text']\n",
        "\n",
        "            # Post-procesamiento\n",
        "            resumen = self.postprocess_summary(resumen)\n",
        "\n",
        "            return {\n",
        "                \"resumen\": resumen,\n",
        "                \"tiempo\": time.time() - start_time,\n",
        "                \"tokens\": len(self.tokenizer.encode(texto_procesado))\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generando resumen: {e}\")\n",
        "            return {\n",
        "                \"resumen\": f\"Error al generar resumen: {str(e)[:100]}...\",\n",
        "                \"tiempo\": time.time() - start_time,\n",
        "                \"tokens\": 0\n",
        "            }\n",
        "\n",
        "    def postprocess_summary(self, resumen: str) -> str:\n",
        "        \"\"\"Post-procesa el resumen para mejor calidad\"\"\"\n",
        "        # Asegurar que termine con punto\n",
        "        if not resumen.endswith('.'):\n",
        "            resumen += '.'\n",
        "\n",
        "        # Capitalizar primera letra\n",
        "        if resumen:\n",
        "            resumen = resumen[0].upper() + resumen[1:]\n",
        "\n",
        "        return resumen\n",
        "\n",
        "    def generate_batch_summaries(self, textos: List[str]) -> List[str]:\n",
        "        \"\"\"Genera múltiples resúmenes en paralelo\"\"\"\n",
        "        resumenes = []\n",
        "\n",
        "        # Procesar en paralelo usando ThreadPoolExecutor\n",
        "        futures = []\n",
        "        for texto in textos:\n",
        "            future = executor.submit(self.generate_summary, texto)\n",
        "            futures.append(future)\n",
        "\n",
        "        # Recopilar resultados\n",
        "        for future in futures:\n",
        "            try:\n",
        "                resultado = future.result(timeout=30)\n",
        "                resumenes.append(resultado[\"resumen\"])\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error en batch processing: {e}\")\n",
        "                resumenes.append(f\"Error procesando texto: {str(e)[:50]}...\")\n",
        "\n",
        "        return resumenes\n",
        "\n",
        "# Instancia global del resumidor\n",
        "logger.info(\"🚀 Inicializando servicio de resumen...\")\n",
        "summarizer_service = OptimizedSummarizer()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Endpoint de salud\"\"\"\n",
        "    return {\n",
        "        \"status\": \"active\",\n",
        "        \"service\": \"Servicio de Resumen Optimizado\",\n",
        "        \"version\": \"2.0.0\",\n",
        "        \"model\": \"facebook/bart-large-cnn\",\n",
        "        \"timestamp\": time.time()\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Check de salud detallado\"\"\"\n",
        "    try:\n",
        "        # Test rápido del modelo\n",
        "        test_result = summarizer_service.generate_summary(\n",
        "            \"Esta es una prueba del sistema de resumen automático para verificar que funciona correctamente.\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"status\": \"healthy\",\n",
        "            \"model_loaded\": summarizer_service.summarizer is not None,\n",
        "            \"test_time\": test_result[\"tiempo\"],\n",
        "            \"memory_usage\": f\"{torch.cuda.memory_allocated() / 1024**2:.1f}MB\" if torch.cuda.is_available() else \"CPU mode\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"unhealthy\", \"error\": str(e)}\n",
        "\n",
        "@app.post(\"/resumir\", response_model=ResumenResponse)\n",
        "async def resumir(texto: Texto):\n",
        "    \"\"\"Endpoint principal para resumir un texto individual\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"📝 Procesando texto de {len(texto.contenido)} caracteres\")\n",
        "\n",
        "        # Generar resumen\n",
        "        resultado = summarizer_service.generate_summary(texto.contenido)\n",
        "\n",
        "        return ResumenResponse(\n",
        "            resumen=resultado[\"resumen\"],\n",
        "            tiempo_procesamiento=resultado[\"tiempo\"],\n",
        "            tokens_procesados=resultado[\"tokens\"]\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Error en /resumir: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error procesando resumen: {str(e)}\")\n",
        "\n",
        "@app.post(\"/resumir_batch\", response_model=ResumenesBatchResponse)\n",
        "async def resumir_batch(textos: TextosBatch):\n",
        "    \"\"\"Endpoint optimizado para procesar múltiples textos en paralelo\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        logger.info(f\"📚 Procesando batch de {len(textos.contenidos)} textos\")\n",
        "\n",
        "        # Procesar en paralelo\n",
        "        resumenes = summarizer_service.generate_batch_summaries(textos.contenidos)\n",
        "\n",
        "        tiempo_total = time.time() - start_time\n",
        "        logger.info(f\"✅ Batch completado en {tiempo_total:.2f}s\")\n",
        "\n",
        "        return ResumenesBatchResponse(\n",
        "            resumenes=resumenes,\n",
        "            tiempo_total=tiempo_total,\n",
        "            articulos_procesados=len(resumenes)\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Error en /resumir_batch: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error procesando batch: {str(e)}\")\n",
        "\n",
        "@app.post(\"/resumir_parrafos\")\n",
        "async def resumir_parrafos(texto: Texto):\n",
        "    \"\"\"Endpoint para resumir párrafo por párrafo (método original mejorado)\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Dividir en párrafos y filtrar\n",
        "        parrafos = [p.strip() for p in texto.contenido.split('\\n') if len(p.strip()) > 20]\n",
        "\n",
        "        if not parrafos:\n",
        "            return {\"resumen\": \"No se encontraron párrafos válidos para resumir.\"}\n",
        "\n",
        "        logger.info(f\"📄 Procesando {len(parrafos)} párrafos\")\n",
        "\n",
        "        # Procesar párrafos en paralelo\n",
        "        resumenes_parrafos = summarizer_service.generate_batch_summaries(parrafos)\n",
        "\n",
        "        # Unir resúmenes\n",
        "        resumen_final = \"\\n\\n\".join(resumenes_parrafos)\n",
        "\n",
        "        tiempo_total = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"resumen\": resumen_final,\n",
        "            \"parrafos_procesados\": len(parrafos),\n",
        "            \"tiempo_total\": tiempo_total\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Error en /resumir_parrafos: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error procesando párrafos: {str(e)}\")\n",
        "\n",
        "@app.get(\"/stats\")\n",
        "async def get_stats():\n",
        "    \"\"\"Estadísticas del servicio\"\"\"\n",
        "    return {\n",
        "        \"model_name\": summarizer_service.model_name,\n",
        "        \"device\": \"CPU\" if summarizer_service.device == -1 else f\"GPU:{summarizer_service.device}\",\n",
        "        \"max_input_length\": summarizer_service.max_input_length,\n",
        "        \"executor_threads\": executor._max_workers,\n",
        "        \"memory_info\": {\n",
        "            \"cuda_available\": torch.cuda.is_available(),\n",
        "            \"cuda_memory\": f\"{torch.cuda.memory_allocated() / 1024**2:.1f}MB\" if torch.cuda.is_available() else \"N/A\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Manejo de memoria\n",
        "@app.post(\"/clear_cache\")\n",
        "async def clear_cache():\n",
        "    \"\"\"Limpia la caché para liberar memoria\"\"\"\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return {\"status\": \"Cache cleared successfully\"}\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "# Funciones de utilidad para servidor\n",
        "def find_free_port(start_port: int = 8000, max_attempts: int = 10) -> int:\n",
        "    \"\"\"Encuentra un puerto libre\"\"\"\n",
        "    import socket\n",
        "    for port in range(start_port, start_port + max_attempts):\n",
        "        try:\n",
        "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "                s.bind(('0.0.0.0', port))\n",
        "                logger.info(f\"✅ Puerto {port} disponible\")\n",
        "                return port\n",
        "        except OSError:\n",
        "            logger.warning(f\"⚠️ Puerto {port} ocupado\")\n",
        "    raise RuntimeError(\"No se encontró puerto libre\")\n",
        "\n",
        "def setup_ngrok_with_port(port: int):\n",
        "    \"\"\"Configura ngrok con manejo de errores mejorado\"\"\"\n",
        "    try:\n",
        "        # Tu token de ngrok\n",
        "        ngrok.set_auth_token(\"\") # colocar token de ngrok aquí\n",
        "\n",
        "        # Configurar túnel\n",
        "        public_url = ngrok.connect(port)\n",
        "        logger.info(f\"🌐 URL pública de ngrok: {public_url}\")\n",
        "\n",
        "        # Guardar URL para referencia\n",
        "        with open('/content/ngrok_url.txt', 'w') as f:\n",
        "            f.write(str(public_url))\n",
        "\n",
        "        # Mostrar información útil\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"🚀 SERVIDOR LISTO\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"📡 URL Local: http://localhost:{port}\")\n",
        "        print(f\"🌐 URL Pública: {public_url}\")\n",
        "        print(f\"📋 Documentación: {public_url}/docs\")\n",
        "        print(f\"❤️ Health Check: {public_url}/health\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        return public_url\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Error configurando ngrok: {e}\")\n",
        "        return None\n",
        "\n",
        "def start_server():\n",
        "    \"\"\"Inicia el servidor con configuración optimizada\"\"\"\n",
        "    try:\n",
        "        logger.info(\"🔧 Configurando servidor...\")\n",
        "\n",
        "        # Encontrar puerto libre\n",
        "        port = find_free_port(8000, 10)\n",
        "\n",
        "        # Configurar ngrok\n",
        "        logger.info(f\"🌐 Configurando ngrok en puerto {port}...\")\n",
        "        ngrok_url = setup_ngrok_with_port(port)\n",
        "\n",
        "        if not ngrok_url:\n",
        "            logger.warning(\"⚠️ No se pudo configurar ngrok, solo acceso local disponible\")\n",
        "\n",
        "        # Configurar uvicorn\n",
        "        config = uvicorn.Config(\n",
        "            app,\n",
        "            host=\"0.0.0.0\",\n",
        "            port=port,\n",
        "            log_level=\"info\",\n",
        "            access_log=True,\n",
        "            reload=False,\n",
        "            workers=1  # Un solo worker para evitar problemas con el modelo\n",
        "        )\n",
        "\n",
        "        server = uvicorn.Server(config)\n",
        "\n",
        "        logger.info(f\"🚀 Iniciando servidor en puerto {port}...\")\n",
        "        server.run()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.info(\"🛑 Servidor detenido por el usuario\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Error fatal del servidor: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        # Limpiar recursos\n",
        "        try:\n",
        "            ngrok.disconnect(ngrok.get_tunnels()[0].public_url)\n",
        "            ngrok.kill()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Middleware para logging de requests\n",
        "@app.middleware(\"http\")\n",
        "async def log_requests(request, call_next):\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = await call_next(request)\n",
        "\n",
        "    process_time = time.time() - start_time\n",
        "    logger.info(f\"📊 {request.method} {request.url.path} - {response.status_code} - {process_time:.3f}s\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Punto de entrada\n",
        "if __name__ == \"__main__\":\n",
        "    # Configurar asyncio para Colab\n",
        "    try:\n",
        "        nest_asyncio.apply()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    start_server()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Uguk2E1_yXL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
