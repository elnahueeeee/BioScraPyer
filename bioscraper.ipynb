{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmnKBNn2wXWN",
        "outputId": "4372300b-1b8c-4c98-8635-0499babaaa3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gsx_2JCwNXO",
        "outputId": "97ccfd46-dd48-4860-8867-83f40eff67bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-238' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-848922167.py\", line 411, in start_server\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üöÄ SERVIDOR LISTO\n",
            "============================================================\n",
            "üì° URL Local: http://localhost:8000\n",
            "üåê URL P√∫blica: NgrokTunnel: \"https://8efcd9ae6d70.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "üìã Documentaci√≥n: NgrokTunnel: \"https://8efcd9ae6d70.ngrok-free.app\" -> \"http://localhost:8000\"/docs\n",
            "‚ù§Ô∏è Health Check: NgrokTunnel: \"https://8efcd9ae6d70.ngrok-free.app\" -> \"http://localhost:8000\"/health\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [476]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2800:a4:167d:1100:e1aa:1ccc:ba1b:f7f6:0 - \"POST /resumir HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Error generando resumen: Already borrowed\n",
            "ERROR:__main__:Error generando resumen: Already borrowed\n",
            "ERROR:__main__:Error generando resumen: Already borrowed\n",
            "ERROR:__main__:Error generando resumen: Already borrowed\n",
            "ERROR:__main__:Error en batch processing: \n",
            "ERROR:__main__:Error en batch processing: \n",
            "ERROR:__main__:Error en batch processing: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2800:a4:167d:1100:e1aa:1ccc:ba1b:f7f6:0 - \"POST /resumir_batch HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import nest_asyncio\n",
        "import time\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "from typing import List, Optional\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gc\n",
        "\n",
        "# Configuraciones\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Evitar warnings\n",
        "\n",
        "# Logging mejorado\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('/content/resumen_server.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Modelos Pydantic\n",
        "class Texto(BaseModel):\n",
        "    contenido: str = Field(..., min_length=10, max_length=10000)\n",
        "\n",
        "class TextosBatch(BaseModel):\n",
        "    contenidos: List[str] = Field(..., min_items=1, max_items=20)\n",
        "\n",
        "class ResumenResponse(BaseModel):\n",
        "    resumen: str\n",
        "    tiempo_procesamiento: float\n",
        "    tokens_procesados: int\n",
        "\n",
        "class ResumenesBatchResponse(BaseModel):\n",
        "    resumenes: List[str]\n",
        "    tiempo_total: float\n",
        "    articulos_procesados: int\n",
        "\n",
        "# Configuraci√≥n de la aplicaci√≥n\n",
        "app = FastAPI(\n",
        "    title=\"Servicio de Resumen Optimizado\",\n",
        "    description=\"API optimizada para res√∫menes de noticias sobre cambio clim√°tico\",\n",
        "    version=\"2.0.0\"\n",
        ")\n",
        "\n",
        "# CORS mejorado\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Pool de threads para procesamiento paralelo\n",
        "executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "class OptimizedSummarizer:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/bart-large-cnn\"\n",
        "        self.summarizer = None\n",
        "        self.tokenizer = None\n",
        "        self.device = -1  # Usar CPU por defecto en Colab\n",
        "        self.max_input_length = 1024\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Carga el modelo de manera optimizada\"\"\"\n",
        "        try:\n",
        "            logger.info(\"üîÑ Cargando modelo de resumen...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Cargar tokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "            # Cargar modelo con configuraciones optimizadas\n",
        "            self.summarizer = pipeline(\n",
        "                \"summarization\",\n",
        "                model=self.model_name,\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=self.device,\n",
        "                framework=\"pt\",\n",
        "                clean_up_tokenization_spaces=True\n",
        "            )\n",
        "\n",
        "            load_time = time.time() - start_time\n",
        "            logger.info(f\"‚úÖ Modelo cargado en {load_time:.2f} segundos\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Error cargando modelo: {e}\")\n",
        "            raise\n",
        "\n",
        "    def preprocess_text(self, texto: str) -> str:\n",
        "        \"\"\"Preprocesa el texto para mejor calidad de resumen\"\"\"\n",
        "        # Limpiar texto\n",
        "        texto = texto.strip()\n",
        "\n",
        "        # Remover l√≠neas muy cortas o repetitivas\n",
        "        lineas = [linea.strip() for linea in texto.split('\\n') if len(linea.strip()) > 20]\n",
        "        texto_limpio = ' '.join(lineas)\n",
        "\n",
        "        # Truncar si es necesario\n",
        "        tokens = self.tokenizer.encode(texto_limpio, truncation=True, max_length=self.max_input_length)\n",
        "        texto_final = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "\n",
        "        return texto_final\n",
        "\n",
        "    def generate_summary(self, texto: str) -> dict:\n",
        "        \"\"\"Genera un resumen optimizado\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Preprocesar texto\n",
        "            texto_procesado = self.preprocess_text(texto)\n",
        "\n",
        "            if len(texto_procesado.strip()) < 50:\n",
        "                return {\n",
        "                    \"resumen\": \"Texto demasiado corto para resumir.\",\n",
        "                    \"tiempo\": time.time() - start_time,\n",
        "                    \"tokens\": 0\n",
        "                }\n",
        "\n",
        "            # Calcular longitudes din√°micamente\n",
        "            texto_length = len(texto_procesado.split())\n",
        "            max_length = min(180, max(60, texto_length // 3))\n",
        "            min_length = min(60, max(30, texto_length // 6))\n",
        "\n",
        "            # Generar resumen con par√°metros optimizados\n",
        "            resumen_result = self.summarizer(\n",
        "                texto_procesado,\n",
        "                max_length=max_length,\n",
        "                min_length=min_length,\n",
        "                do_sample=False,\n",
        "                length_penalty=1.5,\n",
        "                no_repeat_ngram_size=3,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            resumen = resumen_result[0]['summary_text']\n",
        "\n",
        "            # Post-procesamiento\n",
        "            resumen = self.postprocess_summary(resumen)\n",
        "\n",
        "            return {\n",
        "                \"resumen\": resumen,\n",
        "                \"tiempo\": time.time() - start_time,\n",
        "                \"tokens\": len(self.tokenizer.encode(texto_procesado))\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generando resumen: {e}\")\n",
        "            return {\n",
        "                \"resumen\": f\"Error al generar resumen: {str(e)[:100]}...\",\n",
        "                \"tiempo\": time.time() - start_time,\n",
        "                \"tokens\": 0\n",
        "            }\n",
        "\n",
        "    def postprocess_summary(self, resumen: str) -> str:\n",
        "        \"\"\"Post-procesa el resumen para mejor calidad\"\"\"\n",
        "        # Asegurar que termine con punto\n",
        "        if not resumen.endswith('.'):\n",
        "            resumen += '.'\n",
        "\n",
        "        # Capitalizar primera letra\n",
        "        if resumen:\n",
        "            resumen = resumen[0].upper() + resumen[1:]\n",
        "\n",
        "        return resumen\n",
        "\n",
        "    def generate_batch_summaries(self, textos: List[str]) -> List[str]:\n",
        "        \"\"\"Genera m√∫ltiples res√∫menes en paralelo\"\"\"\n",
        "        resumenes = []\n",
        "\n",
        "        # Procesar en paralelo usando ThreadPoolExecutor\n",
        "        futures = []\n",
        "        for texto in textos:\n",
        "            future = executor.submit(self.generate_summary, texto)\n",
        "            futures.append(future)\n",
        "\n",
        "        # Recopilar resultados\n",
        "        for future in futures:\n",
        "            try:\n",
        "                resultado = future.result(timeout=30)\n",
        "                resumenes.append(resultado[\"resumen\"])\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error en batch processing: {e}\")\n",
        "                resumenes.append(f\"Error procesando texto: {str(e)[:50]}...\")\n",
        "\n",
        "        return resumenes\n",
        "\n",
        "# Instancia global del resumidor\n",
        "logger.info(\"üöÄ Inicializando servicio de resumen...\")\n",
        "summarizer_service = OptimizedSummarizer()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Endpoint de salud\"\"\"\n",
        "    return {\n",
        "        \"status\": \"active\",\n",
        "        \"service\": \"Servicio de Resumen Optimizado\",\n",
        "        \"version\": \"2.0.0\",\n",
        "        \"model\": \"facebook/bart-large-cnn\",\n",
        "        \"timestamp\": time.time()\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Check de salud detallado\"\"\"\n",
        "    try:\n",
        "        # Test r√°pido del modelo\n",
        "        test_result = summarizer_service.generate_summary(\n",
        "            \"Esta es una prueba del sistema de resumen autom√°tico para verificar que funciona correctamente.\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"status\": \"healthy\",\n",
        "            \"model_loaded\": summarizer_service.summarizer is not None,\n",
        "            \"test_time\": test_result[\"tiempo\"],\n",
        "            \"memory_usage\": f\"{torch.cuda.memory_allocated() / 1024**2:.1f}MB\" if torch.cuda.is_available() else \"CPU mode\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"unhealthy\", \"error\": str(e)}\n",
        "\n",
        "@app.post(\"/resumir\", response_model=ResumenResponse)\n",
        "async def resumir(texto: Texto):\n",
        "    \"\"\"Endpoint principal para resumir un texto individual\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"üìù Procesando texto de {len(texto.contenido)} caracteres\")\n",
        "\n",
        "        # Generar resumen\n",
        "        resultado = summarizer_service.generate_summary(texto.contenido)\n",
        "\n",
        "        return ResumenResponse(\n",
        "            resumen=resultado[\"resumen\"],\n",
        "            tiempo_procesamiento=resultado[\"tiempo\"],\n",
        "            tokens_procesados=resultado[\"tokens\"]\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error en /resumir: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error procesando resumen: {str(e)}\")\n",
        "\n",
        "@app.post(\"/resumir_batch\", response_model=ResumenesBatchResponse)\n",
        "async def resumir_batch(textos: TextosBatch):\n",
        "    \"\"\"Endpoint optimizado para procesar m√∫ltiples textos en paralelo\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        logger.info(f\"üìö Procesando batch de {len(textos.contenidos)} textos\")\n",
        "\n",
        "        # Procesar en paralelo\n",
        "        resumenes = summarizer_service.generate_batch_summaries(textos.contenidos)\n",
        "\n",
        "        tiempo_total = time.time() - start_time\n",
        "        logger.info(f\"‚úÖ Batch completado en {tiempo_total:.2f}s\")\n",
        "\n",
        "        return ResumenesBatchResponse(\n",
        "            resumenes=resumenes,\n",
        "            tiempo_total=tiempo_total,\n",
        "            articulos_procesados=len(resumenes)\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error en /resumir_batch: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error procesando batch: {str(e)}\")\n",
        "\n",
        "@app.post(\"/resumir_parrafos\")\n",
        "async def resumir_parrafos(texto: Texto):\n",
        "    \"\"\"Endpoint para resumir p√°rrafo por p√°rrafo (m√©todo original mejorado)\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Dividir en p√°rrafos y filtrar\n",
        "        parrafos = [p.strip() for p in texto.contenido.split('\\n') if len(p.strip()) > 20]\n",
        "\n",
        "        if not parrafos:\n",
        "            return {\"resumen\": \"No se encontraron p√°rrafos v√°lidos para resumir.\"}\n",
        "\n",
        "        logger.info(f\"üìÑ Procesando {len(parrafos)} p√°rrafos\")\n",
        "\n",
        "        # Procesar p√°rrafos en paralelo\n",
        "        resumenes_parrafos = summarizer_service.generate_batch_summaries(parrafos)\n",
        "\n",
        "        # Unir res√∫menes\n",
        "        resumen_final = \"\\n\\n\".join(resumenes_parrafos)\n",
        "\n",
        "        tiempo_total = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"resumen\": resumen_final,\n",
        "            \"parrafos_procesados\": len(parrafos),\n",
        "            \"tiempo_total\": tiempo_total\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error en /resumir_parrafos: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error procesando p√°rrafos: {str(e)}\")\n",
        "\n",
        "@app.get(\"/stats\")\n",
        "async def get_stats():\n",
        "    \"\"\"Estad√≠sticas del servicio\"\"\"\n",
        "    return {\n",
        "        \"model_name\": summarizer_service.model_name,\n",
        "        \"device\": \"CPU\" if summarizer_service.device == -1 else f\"GPU:{summarizer_service.device}\",\n",
        "        \"max_input_length\": summarizer_service.max_input_length,\n",
        "        \"executor_threads\": executor._max_workers,\n",
        "        \"memory_info\": {\n",
        "            \"cuda_available\": torch.cuda.is_available(),\n",
        "            \"cuda_memory\": f\"{torch.cuda.memory_allocated() / 1024**2:.1f}MB\" if torch.cuda.is_available() else \"N/A\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Manejo de memoria\n",
        "@app.post(\"/clear_cache\")\n",
        "async def clear_cache():\n",
        "    \"\"\"Limpia la cach√© para liberar memoria\"\"\"\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return {\"status\": \"Cache cleared successfully\"}\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "# Funciones de utilidad para servidor\n",
        "def find_free_port(start_port: int = 8000, max_attempts: int = 10) -> int:\n",
        "    \"\"\"Encuentra un puerto libre\"\"\"\n",
        "    import socket\n",
        "    for port in range(start_port, start_port + max_attempts):\n",
        "        try:\n",
        "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "                s.bind(('0.0.0.0', port))\n",
        "                logger.info(f\"‚úÖ Puerto {port} disponible\")\n",
        "                return port\n",
        "        except OSError:\n",
        "            logger.warning(f\"‚ö†Ô∏è Puerto {port} ocupado\")\n",
        "    raise RuntimeError(\"No se encontr√≥ puerto libre\")\n",
        "\n",
        "def setup_ngrok_with_port(port: int):\n",
        "    \"\"\"Configura ngrok con manejo de errores mejorado\"\"\"\n",
        "    try:\n",
        "        # Tu token de ngrok\n",
        "        ngrok.set_auth_token(\"\") # colocar token de ngrok aqu√≠\n",
        "\n",
        "        # Configurar t√∫nel\n",
        "        public_url = ngrok.connect(port)\n",
        "        logger.info(f\"üåê URL p√∫blica de ngrok: {public_url}\")\n",
        "\n",
        "        # Guardar URL para referencia\n",
        "        with open('/content/ngrok_url.txt', 'w') as f:\n",
        "            f.write(str(public_url))\n",
        "\n",
        "        # Mostrar informaci√≥n √∫til\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üöÄ SERVIDOR LISTO\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"üì° URL Local: http://localhost:{port}\")\n",
        "        print(f\"üåê URL P√∫blica: {public_url}\")\n",
        "        print(f\"üìã Documentaci√≥n: {public_url}/docs\")\n",
        "        print(f\"‚ù§Ô∏è Health Check: {public_url}/health\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        return public_url\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error configurando ngrok: {e}\")\n",
        "        return None\n",
        "\n",
        "def start_server():\n",
        "    \"\"\"Inicia el servidor con configuraci√≥n optimizada\"\"\"\n",
        "    try:\n",
        "        logger.info(\"üîß Configurando servidor...\")\n",
        "\n",
        "        # Encontrar puerto libre\n",
        "        port = find_free_port(8000, 10)\n",
        "\n",
        "        # Configurar ngrok\n",
        "        logger.info(f\"üåê Configurando ngrok en puerto {port}...\")\n",
        "        ngrok_url = setup_ngrok_with_port(port)\n",
        "\n",
        "        if not ngrok_url:\n",
        "            logger.warning(\"‚ö†Ô∏è No se pudo configurar ngrok, solo acceso local disponible\")\n",
        "\n",
        "        # Configurar uvicorn\n",
        "        config = uvicorn.Config(\n",
        "            app,\n",
        "            host=\"0.0.0.0\",\n",
        "            port=port,\n",
        "            log_level=\"info\",\n",
        "            access_log=True,\n",
        "            reload=False,\n",
        "            workers=1  # Un solo worker para evitar problemas con el modelo\n",
        "        )\n",
        "\n",
        "        server = uvicorn.Server(config)\n",
        "\n",
        "        logger.info(f\"üöÄ Iniciando servidor en puerto {port}...\")\n",
        "        server.run()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.info(\"üõë Servidor detenido por el usuario\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error fatal del servidor: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        # Limpiar recursos\n",
        "        try:\n",
        "            ngrok.disconnect(ngrok.get_tunnels()[0].public_url)\n",
        "            ngrok.kill()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Middleware para logging de requests\n",
        "@app.middleware(\"http\")\n",
        "async def log_requests(request, call_next):\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = await call_next(request)\n",
        "\n",
        "    process_time = time.time() - start_time\n",
        "    logger.info(f\"üìä {request.method} {request.url.path} - {response.status_code} - {process_time:.3f}s\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Punto de entrada\n",
        "if __name__ == \"__main__\":\n",
        "    # Configurar asyncio para Colab\n",
        "    try:\n",
        "        nest_asyncio.apply()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    start_server()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Uguk2E1_yXL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
