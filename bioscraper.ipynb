{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmnKBNn2wXWN",
        "outputId": "8e9e8d04-47ac-46b6-ef95-ec2e844c1a86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gsx_2JCwNXO",
        "outputId": "21c38dcd-6373-4c62-8be7-22037d86ffea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-172' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2987199528.py\", line 336, in start_server\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üöÄ SERVIDOR LISTO\n",
            "============================================================\n",
            "üì° URL Local: http://localhost:8000\n",
            "üåê URL P√∫blica: NgrokTunnel: \"https://36760803ed93.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "üìã Documentaci√≥n: NgrokTunnel: \"https://36760803ed93.ngrok-free.app\" -> \"http://localhost:8000\"/docs\n",
            "‚ù§Ô∏è Health Check: NgrokTunnel: \"https://36760803ed93.ngrok-free.app\" -> \"http://localhost:8000\"/health\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [722]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2800:a4:1609:ec00:59a7:341c:eece:37e2:0 - \"POST /resumir HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [722]\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import nest_asyncio\n",
        "import time\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "from typing import List, Optional\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gc\n",
        "\n",
        "# Configuraciones\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Evitar warnings\n",
        "\n",
        "# Logging mejorado\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('/content/resumen_server.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Modelos Pydantic\n",
        "class Texto(BaseModel):\n",
        "    contenido: str = Field(..., min_length=10, max_length=10000)\n",
        "\n",
        "class TextosBatch(BaseModel):\n",
        "    contenidos: List[str] = Field(..., min_items=1, max_items=20)\n",
        "\n",
        "class ResumenResponse(BaseModel):\n",
        "    resumen: str\n",
        "    tiempo_procesamiento: float\n",
        "    tokens_procesados: int\n",
        "\n",
        "class ResumenesBatchResponse(BaseModel):\n",
        "    resumenes: List[str]\n",
        "    tiempo_total: float\n",
        "    articulos_procesados: int\n",
        "\n",
        "# Configuraci√≥n de la aplicaci√≥n\n",
        "app = FastAPI(\n",
        "    title=\"Servicio de Resumen Optimizado\",\n",
        "    description=\"API optimizada para res√∫menes de noticias sobre cambio clim√°tico\",\n",
        "    version=\"2.0.0\"\n",
        ")\n",
        "\n",
        "# CORS mejorado\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Pool de threads para procesamiento paralelo\n",
        "executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "class OptimizedSummarizer:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/bart-large-cnn\"\n",
        "        self.summarizer = None\n",
        "        self.tokenizer = None\n",
        "        self.device = 0 if torch.cuda.is_available() else -1  # GPU si disponible\n",
        "        self.max_input_length = 1024\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            logger.info(\"üîÑ Cargando modelo de resumen...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "            self.summarizer = pipeline(\n",
        "                \"summarization\",\n",
        "                model=self.model_name,\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=self.device,\n",
        "                framework=\"pt\",\n",
        "                clean_up_tokenization_spaces=True\n",
        "            )\n",
        "\n",
        "            load_time = time.time() - start_time\n",
        "            logger.info(f\"‚úÖ Modelo cargado en {load_time:.2f} segundos usando {'GPU' if self.device != -1 else 'CPU'}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Error cargando modelo: {e}\")\n",
        "            raise\n",
        "\n",
        "    def preprocess_text(self, texto: str) -> str:\n",
        "        texto = texto.strip()\n",
        "        lineas = [linea.strip() for linea in texto.split('\\n') if len(linea.strip()) > 20]\n",
        "        texto_limpio = ' '.join(lineas)\n",
        "        tokens = self.tokenizer.encode(texto_limpio, truncation=True, max_length=self.max_input_length)\n",
        "        return self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "\n",
        "    def generate_summary(self, texto: str):\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            texto = self.preprocess_text(texto)\n",
        "            tokens = self.tokenizer.encode(texto)\n",
        "            resumen = self.summarizer(\n",
        "                texto,\n",
        "                max_length=130,\n",
        "                min_length=30,\n",
        "                do_sample=False\n",
        "            )[0]['summary_text']\n",
        "            resumen_final = self.postprocess_summary(resumen)\n",
        "            return {\n",
        "                \"resumen\": resumen_final,\n",
        "                \"tiempo_procesamiento\": time.time() - start_time,  # Fixed key name\n",
        "                \"tokens_procesados\": len(tokens)  # Fixed key name\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generando resumen: {e}\")\n",
        "            return {\n",
        "                \"resumen\": f\"Error al generar resumen: {str(e)}\",\n",
        "                \"tiempo_procesamiento\": 0,\n",
        "                \"tokens_procesados\": 0\n",
        "            }\n",
        "\n",
        "    def postprocess_summary(self, resumen: str) -> str:\n",
        "        if not resumen.endswith('.'):\n",
        "            resumen += '.'\n",
        "        if resumen:\n",
        "            resumen = resumen[0].upper() + resumen[1:]\n",
        "        return resumen\n",
        "\n",
        "    def generate_batch_summaries(self, textos: List[str]) -> List[dict]:\n",
        "        futures = [executor.submit(self.generate_summary, texto) for texto in textos]\n",
        "        resultados = []\n",
        "        for future in futures:\n",
        "            try:\n",
        "                resultados.append(future.result(timeout=30))\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error en batch processing: {e}\")\n",
        "                resultados.append({\n",
        "                    \"resumen\": f\"Error: {str(e)}\",\n",
        "                    \"tiempo_procesamiento\": 0,\n",
        "                    \"tokens_procesados\": 0\n",
        "                })\n",
        "        return resultados\n",
        "\n",
        "\n",
        "# Instancia global del resumidor\n",
        "logger.info(\"üöÄ Inicializando servicio de resumen...\")\n",
        "summarizer_service = OptimizedSummarizer()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Endpoint de salud\"\"\"\n",
        "    return {\n",
        "        \"status\": \"active\",\n",
        "        \"service\": \"Servicio de Resumen Optimizado\",\n",
        "        \"version\": \"2.0.0\",\n",
        "        \"model\": \"facebook/bart-large-cnn\",\n",
        "        \"timestamp\": time.time()\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Check de salud detallado\"\"\"\n",
        "    try:\n",
        "        # Test r√°pido del modelo\n",
        "        test_result = summarizer_service.generate_summary(\n",
        "            \"Esta es una prueba del sistema de resumen autom√°tico para verificar que funciona correctamente.\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"status\": \"healthy\",\n",
        "            \"model_loaded\": summarizer_service.summarizer is not None,\n",
        "            \"test_time\": test_result[\"tiempo_procesamiento\"],\n",
        "            \"memory_usage\": f\"{torch.cuda.memory_allocated() / 1024**2:.1f}MB\" if torch.cuda.is_available() else \"CPU mode\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"unhealthy\", \"error\": str(e)}\n",
        "\n",
        "@app.post(\"/resumir\", response_model=ResumenResponse)\n",
        "async def resumir(texto: Texto):\n",
        "    \"\"\"Endpoint principal para resumir un texto individual\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"üìù Procesando texto de {len(texto.contenido)} caracteres\")\n",
        "\n",
        "        # Generar resumen\n",
        "        resultado = summarizer_service.generate_summary(texto.contenido)\n",
        "        return ResumenResponse(**resultado)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error en /resumir: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error procesando resumen: {str(e)}\")\n",
        "\n",
        "@app.post(\"/resumir_batch\", response_model=ResumenesBatchResponse)\n",
        "async def resumir_batch(req: TextosBatch):\n",
        "    start_time = time.time()\n",
        "    resultados = summarizer_service.generate_batch_summaries(req.contenidos)\n",
        "    return ResumenesBatchResponse(\n",
        "        resumenes=[r[\"resumen\"] for r in resultados],\n",
        "        tiempo_total=time.time() - start_time,\n",
        "        articulos_procesados=len(resultados)\n",
        "    )\n",
        "\n",
        "@app.post(\"/resumir_parrafos\")\n",
        "async def resumir_parrafos(texto: Texto):\n",
        "    \"\"\"Endpoint para resumir p√°rrafo por p√°rrafo (m√©todo original mejorado)\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Dividir en p√°rrafos y filtrar\n",
        "        parrafos = [p.strip() for p in texto.contenido.split('\\n') if len(p.strip()) > 20]\n",
        "\n",
        "        if not parrafos:\n",
        "            return {\"resumen\": \"No se encontraron p√°rrafos v√°lidos para resumir.\"}\n",
        "\n",
        "        logger.info(f\"üìÑ Procesando {len(parrafos)} p√°rrafos\")\n",
        "\n",
        "        # Procesar p√°rrafos en paralelo\n",
        "        resumenes_parrafos = summarizer_service.generate_batch_summaries(parrafos)\n",
        "\n",
        "        # Unir res√∫menes\n",
        "        resumen_final = \"\\n\\n\".join([r[\"resumen\"] for r in resumenes_parrafos])\n",
        "\n",
        "        tiempo_total = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"resumen\": resumen_final,\n",
        "            \"parrafos_procesados\": len(parrafos),\n",
        "            \"tiempo_total\": tiempo_total\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error en /resumir_parrafos: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Error procesando p√°rrafos: {str(e)}\")\n",
        "\n",
        "@app.get(\"/stats\")\n",
        "async def get_stats():\n",
        "    \"\"\"Estad√≠sticas del servicio\"\"\"\n",
        "    return {\n",
        "        \"model_name\": summarizer_service.model_name,\n",
        "        \"device\": \"CPU\" if summarizer_service.device == -1 else f\"GPU:{summarizer_service.device}\",\n",
        "        \"max_input_length\": summarizer_service.max_input_length,\n",
        "        \"executor_threads\": executor._max_workers,\n",
        "        \"memory_info\": {\n",
        "            \"cuda_available\": torch.cuda.is_available(),\n",
        "            \"cuda_memory\": f\"{torch.cuda.memory_allocated() / 1024**2:.1f}MB\" if torch.cuda.is_available() else \"N/A\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Manejo de memoria\n",
        "@app.post(\"/clear_cache\")\n",
        "async def clear_cache():\n",
        "    \"\"\"Limpia la cach√© para liberar memoria\"\"\"\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return {\"status\": \"Cache cleared successfully\"}\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "# Funciones de utilidad para servidor\n",
        "def find_free_port(start_port: int = 8000, max_attempts: int = 10) -> int:\n",
        "    \"\"\"Encuentra un puerto libre\"\"\"\n",
        "    import socket\n",
        "    for port in range(start_port, start_port + max_attempts):\n",
        "        try:\n",
        "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "                s.bind(('0.0.0.0', port))\n",
        "                logger.info(f\"‚úÖ Puerto {port} disponible\")\n",
        "                return port\n",
        "        except OSError:\n",
        "            logger.warning(f\"‚ö†Ô∏è Puerto {port} ocupado\")\n",
        "    raise RuntimeError(\"No se encontr√≥ puerto libre\")\n",
        "\n",
        "def setup_ngrok_with_port(port: int):\n",
        "    \"\"\"Configura ngrok con manejo de errores mejorado\"\"\"\n",
        "    try:\n",
        "        # Tu token de ngrok\n",
        "        ngrok.set_auth_token(\"2xEermx5e5Clyf3vVih326xdKKb_7oaPNcL5tDaT2TLy9vBVF\")\n",
        "\n",
        "        # Configurar t√∫nel\n",
        "        public_url = ngrok.connect(port)\n",
        "        logger.info(f\"üåê URL p√∫blica de ngrok: {public_url}\")\n",
        "\n",
        "        # Guardar URL para referencia\n",
        "        with open('/content/ngrok_url.txt', 'w') as f:\n",
        "            f.write(str(public_url))\n",
        "\n",
        "        # Mostrar informaci√≥n √∫til\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üöÄ SERVIDOR LISTO\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"üì° URL Local: http://localhost:{port}\")\n",
        "        print(f\"üåê URL P√∫blica: {public_url}\")\n",
        "        print(f\"üìã Documentaci√≥n: {public_url}/docs\")\n",
        "        print(f\"‚ù§Ô∏è Health Check: {public_url}/health\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        return public_url\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error configurando ngrok: {e}\")\n",
        "        return None\n",
        "\n",
        "def start_server():\n",
        "    \"\"\"Inicia el servidor con configuraci√≥n optimizada\"\"\"\n",
        "    try:\n",
        "        logger.info(\"üîß Configurando servidor...\")\n",
        "\n",
        "        # Encontrar puerto libre\n",
        "        port = find_free_port(8000, 10)\n",
        "\n",
        "        # Configurar ngrok\n",
        "        logger.info(f\"üåê Configurando ngrok en puerto {port}...\")\n",
        "        ngrok_url = setup_ngrok_with_port(port)\n",
        "\n",
        "        if not ngrok_url:\n",
        "            logger.warning(\"‚ö†Ô∏è No se pudo configurar ngrok, solo acceso local disponible\")\n",
        "\n",
        "        # Configurar uvicorn\n",
        "        config = uvicorn.Config(\n",
        "            app,\n",
        "            host=\"0.0.0.0\",\n",
        "            port=port,\n",
        "            log_level=\"info\",\n",
        "            access_log=True,\n",
        "            reload=False,\n",
        "            workers=1  # Un solo worker para evitar problemas con el modelo\n",
        "        )\n",
        "\n",
        "        server = uvicorn.Server(config)\n",
        "\n",
        "        logger.info(f\"üöÄ Iniciando servidor en puerto {port}...\")\n",
        "        server.run()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.info(\"üõë Servidor detenido por el usuario\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error fatal del servidor: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        # Limpiar recursos\n",
        "        try:\n",
        "            ngrok.disconnect(ngrok.get_tunnels()[0].public_url)\n",
        "            ngrok.kill()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Middleware para logging de requests\n",
        "@app.middleware(\"http\")\n",
        "async def log_requests(request, call_next):\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = await call_next(request)\n",
        "\n",
        "    process_time = time.time() - start_time\n",
        "    logger.info(f\"üìä {request.method} {request.url.path} - {response.status_code} - {process_time:.3f}s\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Punto de entrada\n",
        "if __name__ == \"__main__\":\n",
        "    # Configurar asyncio para Colab\n",
        "    try:\n",
        "        nest_asyncio.apply()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    start_server()"
      ]
    }
  ]
}